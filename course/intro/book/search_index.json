[["index.html", "An Introduction to Data Science About the Authors", " An Introduction to Data Science Ron Smith Tyler Frazier Daniel Vasiliu Tyler W. Davis William &amp; Mary2021-03-03 About the Authors "],["getting-started.html", "Getting Started ", " Getting Started "],["introduction-to-data-science.html", "0.1 Introduction to Data Science", " 0.1 Introduction to Data Science What is Data Science and how did we get here? Data Science is the study (practice) of integrating mathematics, programming, and data to make meaningful inferences about the world around us. We differentiate data science from other fields, such as computer science, statistics and information science. Computer science is the study of algorithms, data structures and programming methodologies, which differentiates from data science as it is missing the meaningful inferences about our world. Statistics is a mathematical discipline concerned with the collection, description, and interpretation of data. This is a closer definition to data science but it is missing the hard data, such as data formats, archiving, and access. Information science is a field primarily concerned with the analysis, collection, classification, manipulation, storage, retrieval, movement, dissemination, and protection of information. The focus for information science is much more on the physical data: library collections, archives, databases, and how this information is consumed by users, but misses the computational and mathematical side of data science. You may think of data science as the intersect of computer science, statistics, and information science. "],["a-brief-history-of-data-science.html", "0.2 A Brief History of Data Science", " 0.2 A Brief History of Data Science TBA "],["a-brief-introduction-to-python.html", "0.3 A Brief Introduction to Python", " 0.3 A Brief Introduction to Python As a data scientist, you will need tools to help you make meaningful inferences about our world from data. One such tool is computer programming. Computer programming comes in a variety of languages and, needless to say, there are a lot to choose from. Thanks to development efforts in numerical and scientific libraries, Python is a good choice. Why Python? First, its considered a high-level language, which means its easierand as a result, generally fasterfor us to read and write. This tends to make development quicker and simpler. Secondly, its an object-oriented language, which has several advantages over structured or functional programming, by simplifying the process of creating programs through the use of specially defined classes of objects; these objects encapsulate a specific set of rules, values and functions that can easily be maintained, reused, shared and scaled. Third, in addition to the standard library, there is a plethora of well-documented and supported third-party packages to help support a variety of projects. Python is actually two things: a language and an interpreter We need to know how to read and write in Python (thats the language) We also need something can understand what we just wrote (thats the translator) The idea that computer programs are a language means we need to get two things straight: semantics and syntax. Syntax is the set of symbols and rules that define terms used collectively to form a correctly structured document. Think of the symbols in our language as an alphabet and the rules act as the grammar and mechanics of the language. In practice, the alphabet is used to form words, words are combined with proper punctuation to form sentences, sentences are combined to form paragraphs, and so forth. A similar progression is done when writing in computer code. Semantics are the meaning (intent or purpose) behind the syntax. We prescribe meaning to words; the same goes for statements written in a program. If you see h e l l o spelled out or you hear someone say hello, you get a sense of greeting; in other words, theres meaning behind those words. We also know that the general greeting, hello, can be spoken in other languages: ni hao (Mandarin), gutentag (German), bonjour (French), shalom (Hebrew), hola (Spanish), ciao (Italian). Notice that these words have the same semantics, but different syntax. "],["the-data-pipeline.html", "1 The Data Pipeline", " 1 The Data Pipeline If the time to build an automated pipeline is less than the time you might spend doing/redoing things manually, its a good time investment The goal of a data pipeline is to automate as much of the process as possible. This saves time and avoids human error when performing the same (or similar) tasks repeatedly. "],["project-idea.html", "1.1 Project Idea", " 1.1 Project Idea "],["data-acquisition.html", "1.2 Data Acquisition", " 1.2 Data Acquisition "],["preprocessing.html", "1.3 Preprocessing", " 1.3 Preprocessing "],["analysis.html", "1.4 Analysis", " 1.4 Analysis "],["presentation.html", "1.5 Presentation", " 1.5 Presentation "],["data-preprocessing.html", "2 Data Preprocessing ", " 2 Data Preprocessing "],["descriptive-analysis.html", "2.1 Descriptive Analysis", " 2.1 Descriptive Analysis This is getting to know your data. Broadly speaking, we can categorize data into three classes: Continuous Ordinal Nominal Continuous data are numerical that can take on a range of values, such as the temperature, or someones income. For continuous data, the magnitude of the numbers, as well as the difference between numbers, is meaningful. Ordinal data are also numerical, but only the order of the values is important, not their difference. An example is the order in which people finish a race (e.g., 1st place, 2nd place, etc.). It doesnt make sense to add these numbers (1st place + 2nd place does not equal 3rd place). Nominal data represent the names of things (e.g., countries, occupations, gender). These data may be represented numerically, but be careful! 2.1.1 Measures of central tendency mean median mode 2.1.2 Measures of variability variance coefficient of variation range quantiles percentiles \\[ \\mathrm{var(x)}, \\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x - x_i)^2 \\] 2.1.3 Visualizations histogram 2.1.4 Transformations logarithm 2.1.5 Correlation correlation coefficient "],["feature-scaling.html", "2.2 Feature Scaling", " 2.2 Feature Scaling In feature scaling, the goal is to transform each variable in a dataset independently, so that afterwards, all variables are represented with a similar range of numbers. standardize (i.e., center and scale) "],["dimensionality-reduction.html", "2.3 Dimensionality Reduction", " 2.3 Dimensionality Reduction In dimensionality reduction, the goal is to transform an N-dimensional dataset (i.e., a dataset with N number of independent variables) into a dataset with less than N dimensions, usually by performing an operation on all variables simultaneously. 2.3.1 Principal component analysis (PCA) The goal of PCA is to find the directions in which the data is most variable. These directions are linear combinations of the original variables. The first PC is the direction along which the data is most variable. The second PC is any direction orthogonal to the first along which the data is most variable. The next PC is any direction orthogonal to the previous PCs along which the data is most variable. 2.3.2 t-distributed Stochastic Neighbor Embedding (tSNE) The goal of tSNE is to find a representation of the data in a lower-dimensional space that, as best as possible, preserves the distribution of local distances in the high dimensional space. "],["modeling-and-validation.html", "3 Modeling and Validation", " 3 Modeling and Validation Data science is the field that cares about finding patterns in data. A goal in discovering these patterns is our ability to reproduce them with a model. What is a model? A model is a mathematical representation of the real world. It can be of whatever phenomenon, process, system, series of observations, or occurrences you want. In many cases, a model is just an equation or an expression. The goal of the model is to estimate natural processes based on physical laws, theories and/or empirical knowledge. In short, a model is a best estimate of some process. A model takes a set of observations and tries to put them together in such a way as to mimic our understanding of our physical laws (e.g., gravitational force, thermal conductivity, fluid dynamics), of our theories (e.g., conflict theory, policy network theory, management theory), and our empirical knowledge. When we take a model as an expression of our understanding of a process, remember that our best insight into how the world works comes from that very first step of the scientific method: our observations. These make up our data. Therefore, these observations should be the basis that informs how we build our models. It all starts with data. What do we do with models? Models are used in two ways: explain/represent past events predict future events "],["linear-models.html", "3.1 Linear models", " 3.1 Linear models linear regression ordinary least squares (OLS) method "],["model-score.html", "3.2 Model Score", " 3.2 Model Score mean squared error (MSE) coefficient of determination (r-squared) "],["model-validation.html", "3.3 Model validation", " 3.3 Model validation internal validity external validity overfit underfit "]]
